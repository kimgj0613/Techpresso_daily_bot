import os
import re
import smtplib
import ssl
import time
from datetime import datetime, timezone, timedelta
from email.message import EmailMessage

import deepl
import feedparser
from bs4 import BeautifulSoup, NavigableString
from dateutil import tz
from weasyprint import HTML


# ======================
# ê¸°ë³¸ ì„¤ì •
# ======================
RSS_URL = os.getenv("RSS_URL", "https://rss.beehiiv.com/feeds/ez2zQOMePQ.xml")

DEEPL_API_KEY = os.getenv("DEEPL_API_KEY")
DEEPL_SERVER_URL = os.getenv("DEEPL_SERVER_URL", "https://api-free.deepl.com")  # Free ê¸°ë³¸

SMTP_HOST = os.getenv("SMTP_HOST", "smtp.gmail.com")
SMTP_PORT = int(os.getenv("SMTP_PORT", "465"))

MAIL_SUBJECT_PREFIX = "â˜• OneSip | Todayâ€™s Tech in One Sip"
MAIL_BODY_LINE = "OneSip â€“ Your daily tech clarity"

BRAND_FROM = "Techpresso"
BRAND_TO = "OneSip"

# ë²ˆì—­ì—ì„œ ì ˆëŒ€ ê±´ë“œë¦¬ë©´ ì•ˆ ë˜ëŠ” ë‹¨ì–´(ë¸Œëœë“œ/ê³ ìœ ëª…ì‚¬)
PROTECT_TERMS = ["OneSip"]

# ë””ë²„ê·¸: GitHub Actionsì—ì„œ HTML/PDFë¥¼ ì•„í‹°íŒ©íŠ¸ë¡œ ë³´ê³  ì‹¶ìœ¼ë©´ 1
DEBUG_DUMP_HTML = os.getenv("DEBUG_DUMP_HTML", "0") == "1"

# âœ… 0ì´ë©´ ë‹¹ì¼, -1ì´ë©´ ì „ë‚ , -2ì´ë©´ ì´í‹€ ì „...
ISSUE_OFFSET_DAYS = int(os.getenv("ISSUE_OFFSET_DAYS", "0"))

KST = tz.gettz("Asia/Seoul")

translator = None
if DEEPL_API_KEY:
    translator = deepl.Translator(DEEPL_API_KEY, server_url=DEEPL_SERVER_URL)


# ======================
# ìœ í‹¸
# ======================
def now_kst():
    return datetime.now(tz=KST)


def get_target_issue_date_kst() -> datetime.date:
    return (now_kst().date() + timedelta(days=ISSUE_OFFSET_DAYS))


def safe_print_deepl_usage(prefix="DeepL usage"):
    if not translator:
        return
    try:
        usage = translator.get_usage()
        print(f"{prefix}: {usage.character.count}/{usage.character.limit}")
    except Exception as e:
        print("DeepL usage check failed:", e)


def _safe_find_parent(node, names):
    """
    BeautifulSoup ë…¸ë“œê°€ ë¶„ë¦¬ë˜ì—ˆê±°ë‚˜( decompose ì´í›„ ),
    ì¼ë¶€ í™˜ê²½ì—ì„œ NavigableString parent ì ‘ê·¼ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆì–´ì„œ
    find_parentëŠ” ë¬´ì¡°ê±´ ì•ˆì „í•˜ê²Œ ê°ì‹¼ë‹¤.
    """
    try:
        if hasattr(node, "find_parent"):
            return node.find_parent(names)
    except Exception:
        return None
    return None


# ======================
# ë²ˆì—­ ë³´í˜¸(placeholder)
# ======================
def protect_terms(text: str):
    """
    OneSip ê°™ì€ ë‹¨ì–´ê°€ ë²ˆì—­ë˜ì§€ ì•Šê²Œ placeholderë¡œ ë°”ê¾¸ê³ ,
    ë²ˆì—­ í›„ ë‹¤ì‹œ ë˜ëŒë¦´ ìˆ˜ ìˆê²Œ ë§¤í•‘ì„ ë°˜í™˜í•œë‹¤.
    """
    if not text:
        return text, {}

    mapping = {}
    out = text

    for term in PROTECT_TERMS:
        placeholder = f"__PROTECT_{re.sub(r'[^A-Za-z0-9]', '', term).upper()}__"
        if term in out:
            out = out.replace(term, placeholder)
            mapping[placeholder] = term

    return out, mapping


def restore_terms(text: str, mapping: dict):
    if not text or not mapping:
        return text
    out = text
    for ph, term in mapping.items():
        out = out.replace(ph, term)
    return out


# ======================
# DeepL ë²ˆì—­ (ê¸´ í…ìŠ¤íŠ¸ ì•ˆì • ì²˜ë¦¬)
# ======================
def _split_by_paragraph(text: str, max_chars: int = 4500):
    text = (text or "").strip()
    if not text:
        return []

    paras = [p.strip() for p in re.split(r"\n{2,}", text) if p.strip()]
    chunks, buf = [], ""

    for p in paras:
        add = p + "\n\n"
        if len(buf) + len(add) <= max_chars:
            buf += add
        else:
            if buf.strip():
                chunks.append(buf.strip())

            if len(add) > max_chars:
                for i in range(0, len(add), max_chars):
                    part = add[i : i + max_chars].strip()
                    if part:
                        chunks.append(part)
                buf = ""
            else:
                buf = add

    if buf.strip():
        chunks.append(buf.strip())

    return chunks


def translate_text(text: str, retries: int = 3) -> str:
    if not text or not text.strip():
        return text
    if translator is None:
        raise ValueError("DEEPL_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    protected, mapping = protect_terms(text)

    chunks = _split_by_paragraph(protected, max_chars=4500)
    if not chunks:
        return text

    out_parts = []
    for ch in chunks:
        translated = None
        for i in range(retries):
            try:
                result = translator.translate_text(
                    ch,
                    target_lang="KO",
                    preserve_formatting=True,
                )
                translated = result.text
                break
            except Exception as e:
                print("DEEPL ERROR:", e)
                time.sleep(2 * (i + 1))
        out_parts.append(translated if translated is not None else ch)

    joined = "\n\n".join(out_parts)
    return restore_terms(joined, mapping)


# ======================
# HTML ì œê±°/ë¸Œëœë”©/ë²ˆì—­
# ======================
REMOVE_KEYWORDS_HEADER_FOOTER = [
    "Join Free",
    "Upgrade",
    "Together with",
    "this is your daily",
    "Not subscribed to",
    "Subscribe for free",
    "Advertise",
    "Feedback",
    "Read Online",
]

REMOVE_SECTION_KEYWORDS = [
    "Want to master the AI tools we cover every day?",
    "ë§¤ì¼ ë‹¤ë£¨ëŠ” AI ë„êµ¬ë¥¼ ë§ˆìŠ¤í„°í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
    "AI ì•„ì¹´ë°ë¯¸",
]

PARTNER_KEYWORDS = [
    "FROM OUR PARTNER",
]


def _text_has_any(text: str, keywords):
    t = (text or "").lower()
    return any(k.lower() in t for k in keywords)


def _match_keyword_count(text: str, keywords) -> int:
    t = (text or "").lower()
    return sum(1 for k in keywords if k.lower() in t)


def _replace_brand_everywhere(soup: BeautifulSoup, old: str, new: str):
    for t in soup.find_all(string=True):
        if old in t:
            t.replace_with(t.replace(old, new))


def _remove_techpresso_header_footer_safely(soup: BeautifulSoup):
    """
    ë„ˆë¬´ í° ì»¨í…Œì´ë„ˆë¥¼ ë‚ ë ¤ì„œ ë³¸ë¬¸ì´ ì‚¬ë¼ì§€ëŠ” ê±¸ ì¤„ì´ê¸° ìœ„í•´
    'ì§§ì€ ë¸”ë¡' ìœ„ì£¼ë¡œë§Œ ì œê±°.
    """
    candidates = soup.find_all(["header", "footer", "div", "section", "table", "tr", "td"])
    for tag in candidates:
        text = tag.get_text(" ", strip=True)
        if not text:
            continue

        kw = _match_keyword_count(text, REMOVE_KEYWORDS_HEADER_FOOTER)
        if kw == 0:
            continue

        if len(text) > 1600:
            continue

        # div/section/table/tr/tdëŠ” ë„ˆë¬´ ê³¼ê°í•˜ë©´ ë³¸ë¬¸ê¹Œì§€ ë‚ ì•„ê°€ì„œ kw>=2ì¼ ë•Œë§Œ
        if tag.name in ["div", "section", "table", "tr", "td"]:
            if kw >= 2:
                tag.decompose()
        else:
            tag.decompose()


def _remove_blocks_containing_keywords_safely(soup: BeautifulSoup, keywords) -> int:
    """
    keywordsê°€ í¬í•¨ëœ ë¸”ë¡ì„ ì‚­ì œí•˜ë˜,
    table/tr/tdë¥¼ ë°”ë¡œ ì§€ìš°ë©´ ë‹¤ë¥¸ ì„¹ì…˜ê¹Œì§€ ê°™ì´ ë‚ ì•„ê°ˆ ìˆ˜ ìˆì–´ì„œ
    ê¸°ë³¸ì€ div/sectionì„ ìš°ì„  ì‚­ì œí•˜ê³ , tableì€ 'ì‘ì€' ê²½ìš°ì—ë§Œ ì‚­ì œ.
    """
    removed = 0
    for node in list(soup.find_all(string=True)):
        # NavigableStringë„ strì˜ subclassë¼ì„œ ê·¸ëƒ¥ str ê²€ì‚¬ë§Œ í•˜ë©´ ìœ„í—˜.
        if not isinstance(node, NavigableString):
            continue

        text = str(node)
        if not text.strip():
            continue
        if not _text_has_any(text, keywords):
            continue

        # 1) div/section ìš°ì„ 
        container = _safe_find_parent(node, ["div", "section"])
        if container:
            txt = container.get_text(" ", strip=True)
            if txt and len(txt) <= 6000:
                container.decompose()
                removed += 1
                continue

        # 2) table (ì§§ì„ ë•Œë§Œ)
        table = _safe_find_parent(node, "table")
        if table:
            txt = table.get_text(" ", strip=True)
            if txt and len(txt) <= 3500:
                table.decompose()
                removed += 1
                continue

        # 3) ë§ˆì§€ë§‰ fallback: ì£¼ë³€ ë¬¸ë‹¨/ì…€ë§Œ ì œê±°
        parent = getattr(node, "parent", None)
        if parent and getattr(parent, "name", "") in ("p", "h1", "h2", "h3", "h4", "td"):
            parent.decompose()
            removed += 1

    return removed


# ----------------------
# (í•µì‹¬) ì²« ë²ˆì§¸ FROM OUR PARTNER ì œê±°: "ë‹¤ìŒ ì²« ì´ëª¨ì§€" ì „ê¹Œì§€ ì‚­ì œ
# ----------------------
# ì´ëª¨ì§€ ëŒ€ëµ ë²”ìœ„(ë‰´ìŠ¤ í—¤ë”ì— ë‚˜ì˜¤ëŠ” ğŸš€ğŸ’¥ğŸ“±ğŸ“ˆğŸ–¥ï¸ğŸ“šğŸğŸ§° ë“± í¬í•¨)
EMOJI_RE = re.compile(
    "["
    "\U0001F300-\U0001F5FF"  # Misc Symbols and Pictographs
    "\U0001F600-\U0001F64F"  # Emoticons
    "\U0001F680-\U0001F6FF"  # Transport & Map
    "\U0001F700-\U0001F77F"
    "\U0001F780-\U0001F7FF"
    "\U0001F800-\U0001F8FF"
    "\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
    "\U0001FA00-\U0001FAFF"  # Symbols and Pictographs Extended-A
    "\u2600-\u26FF"          # Misc symbols
    "\u2700-\u27BF"          # Dingbats
    "]+"
)


def _find_first_partner_marker_node(soup: BeautifulSoup):
    for s in soup.find_all(string=True):
        if not isinstance(s, NavigableString):
            continue
        if "from our partner" in str(s).lower():
            return s
    return None


def _find_first_emoji_node_after(start_node: NavigableString):
    """
    start_node ì´í›„ ë¬¸ì„œ ìˆœì„œì—ì„œ ì²˜ìŒ ì´ëª¨ì§€ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ ë…¸ë“œë¥¼ ì°¾ëŠ”ë‹¤.
    """
    try:
        it = start_node.next_elements
    except Exception:
        return None

    for el in it:
        if not isinstance(el, NavigableString):
            continue
        t = str(el)
        if not t.strip():
            continue
        if EMOJI_RE.search(t):
            return el
    return None


def _remove_first_partner_until_emoji(soup: BeautifulSoup) -> int:
    """
    ì²« ë²ˆì§¸ FROM OUR PARTNER ê°€ ë“±ì¥í•˜ë©´,
    ë‹¤ìŒ ì²« ì´ëª¨ì§€ í…ìŠ¤íŠ¸ ë…¸ë“œê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ DOM ìƒì˜ ìš”ì†Œë“¤ì„ ì œê±°í•œë‹¤.
    (ì´ëª¨ì§€ë¶€í„°ëŠ” ì‚´ë¦°ë‹¤)
    """
    marker = _find_first_partner_marker_node(soup)
    if not marker:
        return 0

    emoji_node = _find_first_emoji_node_after(marker)
    if not emoji_node:
        # ì´ëª¨ì§€ë¥¼ ëª» ì°¾ìœ¼ë©´ ê³¼ê° ì‚­ì œê°€ ìœ„í—˜í•˜ë‹ˆ ì œê±° ì•ˆ í•¨
        return 0

    end_tag = _safe_find_parent(emoji_node, ["td", "div", "p", "h1", "h2", "h3", "h4", "section"])
    if not end_tag:
        return 0

    # start_tagëŠ” markerê°€ ì†í•œ "ì ë‹¹íˆ ì‘ì€" ì»¨í…Œì´ë„ˆë¶€í„° ì¡ëŠ”ë‹¤.
    # (h4/td/div ìˆœìœ¼ë¡œ ì‹œë„)
    start_tag = _safe_find_parent(marker, ["h1", "h2", "h3", "h4", "td", "div", "section"])
    if not start_tag:
        return 0

    # end_tagì˜ ì¡°ìƒì€ ì œê±° ëŒ€ìƒì—ì„œ ì œì™¸(ë¶€ëª¨ë¥¼ ì§€ìš°ë©´ end_tagê¹Œì§€ ê°™ì´ ë‚ ì•„ê°)
    end_ancestors = set()
    cur = end_tag
    while cur is not None:
        end_ancestors.add(cur)
        cur = getattr(cur, "parent", None)

    removed = 0
    # start_tagë¶€í„° end_tag ì§ì „ê¹Œì§€, ë¬¸ì„œ ìˆœì„œìƒ ìš”ì†Œë“¤ì„ ëª¨ì•„ì„œ ì œê±°
    to_kill = []
    for el in start_tag.next_elements:
        if el == end_tag:
            break
        if not hasattr(el, "name"):
            continue  # ë¬¸ìì—´ ë“±
        if el in end_ancestors:
            continue
        # html/bodyëŠ” ì œì™¸
        if getattr(el, "name", "") in ("html", "body"):
            continue
        to_kill.append(el)

    # start_tag ìì²´ë„ ì œê±°(ë‹¨, end_tagì˜ ì¡°ìƒì´ë©´ ì•ˆ ë¨)
    if start_tag not in end_ancestors:
        to_kill.insert(0, start_tag)

    # ì¤‘ë³µ ì œê±°(ê¹Šì€ ìì‹ë¶€í„° ì œê±°ë˜ëŠ” ê±¸ ë§‰ê¸° ìœ„í•´ ê³ ìœ í™”)
    seen = set()
    uniq = []
    for t in to_kill:
        if t in seen:
            continue
        seen.add(t)
        uniq.append(t)

    for t in uniq:
        try:
            t.decompose()
            removed += 1
        except Exception:
            pass

    return removed


def _fix_first_article_alignment(soup: BeautifulSoup):
    """
    ì²« íŒŒíŠ¸ë„ˆ ë¸”ë¡ì„ ì œê±°í•œ ë’¤, ì²« ê¸°ì‚¬(ì´ëª¨ì§€ë¡œ ì‹œì‘)ê°€
    ê°€ìš´ë° ì •ë ¬ì²˜ëŸ¼ ë³´ì´ëŠ” í˜„ìƒì„ ì™„í™”í•˜ê¸° ìœ„í•´
    ì²« ì´ëª¨ì§€ í—¤ë”ì˜ td/ë¶€ëª¨ì˜ center ê´€ë ¨ ì†ì„±ì„ ì œê±°í•˜ê³  leftë¡œ ê°•ì œ.
    """
    # ì²« ì´ëª¨ì§€ í…ìŠ¤íŠ¸ ë…¸ë“œ ì°¾ê¸°
    first_emoji_str = None
    for s in soup.find_all(string=True):
        if not isinstance(s, NavigableString):
            continue
        t = str(s).strip()
        if not t:
            continue
        if EMOJI_RE.search(t):
            first_emoji_str = s
            break

    if not first_emoji_str:
        return

    td = _safe_find_parent(first_emoji_str, "td")
    if not td:
        return

    # td ë° ìƒìœ„ ëª‡ ë‹¨ê³„ì—ì„œ align/styleì˜ center ì œê±°
    cur = td
    for _ in range(5):
        if not cur or not hasattr(cur, "attrs"):
            break

        if cur.has_attr("align") and str(cur["align"]).lower() == "center":
            del cur["align"]

        style = cur.get("style", "")
        if style:
            style2 = re.sub(r"text-align\s*:\s*center\s*;?", "", style, flags=re.I)
            style2 = style2.strip()
            if style2:
                cur["style"] = style2
            else:
                if cur.has_attr("style"):
                    del cur["style"]

        cur = getattr(cur, "parent", None)

    # tdëŠ” leftë¡œ ëª…ì‹œ
    td_style = td.get("style", "")
    if "text-align" not in td_style.lower():
        td["style"] = (td_style + "; " if td_style else "") + "text-align: left !important;"


# ----------------------
# URL í‘œì‹œ ì œê±° + ë§í¬ ìœ ì§€ ë²ˆì—­
# ----------------------
URL_RE = re.compile(r"(https?://\S+|www\.\S+)", re.IGNORECASE)


def remove_visible_urls(soup: BeautifulSoup):
    """
    'í…ìŠ¤íŠ¸ë¡œ ë…¸ì¶œëœ URL'ë§Œ ì œê±°í•´ì„œ PDFì— URLì´ ë³´ì´ì§€ ì•Šê²Œ.
    <a href="...">ëŠ” ê±´ë“œë¦¬ì§€ ì•Šì•„ì„œ ë§í¬ëŠ” ìœ ì§€ë¨.
    """
    for node in list(soup.find_all(string=True)):
        if not isinstance(node, NavigableString):
            continue

        parent = getattr(node, "parent", None)
        parent_name = parent.name if parent else ""
        if parent_name in ("script", "style"):
            continue

        txt = str(node)
        if not txt.strip():
            continue

        if URL_RE.search(txt):
            cleaned = URL_RE.sub("", txt)
            cleaned = re.sub(r"\(\s*\)", "", cleaned)  # ë¹ˆ ê´„í˜¸ ì œê±°
            cleaned = re.sub(r"\s{2,}", " ", cleaned).strip()
            node.replace_with(cleaned)


def translate_text_nodes_inplace(soup: BeautifulSoup):
    """
    HTML íƒœê·¸ êµ¬ì¡°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , í…ìŠ¤íŠ¸ ë…¸ë“œë§Œ ë²ˆì—­.
    => <a href> ë§í¬ ìœ ì§€ + URLì€ ë²ˆì—­/í‘œì‹œí•˜ì§€ ì•ŠìŒ
    """
    translated_nodes = 0

    for node in list(soup.find_all(string=True)):
        if not isinstance(node, NavigableString):
            continue

        parent = getattr(node, "parent", None)
        parent_name = parent.name if parent else ""
        if parent_name in ("script", "style"):
            continue

        # âœ… Trending tools ë“±ì—ì„œ bold/strong(ë„êµ¬ëª…/ê³ ìœ ëª…ì‚¬)ì€ ë²ˆì—­ ì œì™¸
        if parent_name in ("strong", "b"):
            continue

        text = str(node)
        if not text.strip():
            continue

        # URLì´ í…ìŠ¤íŠ¸ë¡œ ë“¤ì–´ìˆë‹¤ë©´(í˜¹ì‹œ ë‚¨ì•˜ìœ¼ë©´) ë²ˆì—­ ì „ì— ì œê±°
        if URL_RE.search(text):
            text = URL_RE.sub("", text)

        # ì˜ì–´ ì•ŒíŒŒë²³ì´ ê±°ì˜ ì—†ìœ¼ë©´ ìŠ¤í‚µ(ìˆ«ì/ê¸°í˜¸/ì´ë¯¸ í•œê¸€ ìœ„ì£¼)
        if len(re.findall(r"[A-Za-z]", text)) < 2:
            continue

        # ë„ˆë¬´ ê¸´ ë…¸ë“œëŠ” ìœ„í—˜/ë¹„ìš© í¼ â†’ ìŠ¤í‚µ
        if len(text) > 2000:
            continue

        translated = translate_text(text)
        if translated is None:
            continue

        node.replace_with(translated)
        translated_nodes += 1

    print("Translated text nodes:", translated_nodes)


def translate_html_preserve_layout(html: str, date_str: str) -> str:
    soup = BeautifulSoup(html, "html.parser")

    # 0) í—¤ë”/í‘¸í„° ì œê±°
    _remove_techpresso_header_footer_safely(soup)

    # 1) ì²« ë²ˆì§¸ FROM OUR PARTNER: ë‹¤ìŒ ì²« ì´ëª¨ì§€ ì „ê¹Œì§€ ì œê±°
    removed_until_emoji = _remove_first_partner_until_emoji(soup)
    if removed_until_emoji:
        print("Main partner ad removed (until emoji):", removed_until_emoji)

    # 2) ê¸°íƒ€ íŒŒíŠ¸ë„ˆ ì„¹ì…˜ ì‚­ì œ(ë‚¨ì•„ìˆëŠ” FROM OUR PARTNERê°€ ë” ìˆìœ¼ë©´)
    removed_partner_keywords = _remove_blocks_containing_keywords_safely(soup, PARTNER_KEYWORDS)
    if removed_partner_keywords:
        print("Blocks removed by keywords (partner):", removed_partner_keywords)

    # 3) AI Academy ì„¹ì…˜ ì‚­ì œ
    removed_ai = _remove_blocks_containing_keywords_safely(soup, REMOVE_SECTION_KEYWORDS)
    if removed_ai:
        print("Blocks removed by keywords (ai-academy):", removed_ai)

    # 4) ê´‘ê³  ì œê±°
    for ad in soup.select("[data-testid='ad'], .sponsor, .advertisement"):
        try:
            ad.decompose()
        except Exception:
            pass

    # 5) ë¸Œëœë”© ì¹˜í™˜ (Techpresso -> OneSip)
    _replace_brand_everywhere(soup, BRAND_FROM, BRAND_TO)

    # 6) ì²« ê¸°ì‚¬ ì–¼ë¼ì¸ ë³´ì •
    _fix_first_article_alignment(soup)

    # 7) URLì„ PDFì— í‘œì‹œí•˜ì§€ ì•Šë„ë¡ í…ìŠ¤íŠ¸ URL ì œê±°
    remove_visible_urls(soup)

    # 8) í…ìŠ¤íŠ¸ ë…¸ë“œë§Œ ë²ˆì—­
    translate_text_nodes_inplace(soup)

    out_html = str(soup)

    # fallback: ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(ê³¼ì‚­ì œ) -> partner ì œê±°ë§Œ ìœ ì§€í•˜ê³  í—¤ë”/í‘¸í„° ì œê±°ëŠ” í’€ì–´ë³¸ë‹¤
    text_len = len(BeautifulSoup(out_html, "html.parser").get_text(" ", strip=True))
    if text_len < 200:
        print("WARNING: HTML too small after cleanup. Falling back without header/footer removal.")
        soup2 = BeautifulSoup(html, "html.parser")

        _remove_first_partner_until_emoji(soup2)
        _remove_blocks_containing_keywords_safely(soup2, PARTNER_KEYWORDS)
        _remove_blocks_containing_keywords_safely(soup2, REMOVE_SECTION_KEYWORDS)

        for ad in soup2.select("[data-testid='ad'], .sponsor, .advertisement"):
            try:
                ad.decompose()
            except Exception:
                pass

        _replace_brand_everywhere(soup2, BRAND_FROM, BRAND_TO)
        _fix_first_article_alignment(soup2)
        remove_visible_urls(soup2)
        translate_text_nodes_inplace(soup2)

        out_html = str(soup2)

    if DEBUG_DUMP_HTML:
        with open(f"debug_onesip_inner_{date_str}.html", "w", encoding="utf-8") as f:
            f.write(out_html)
        print("Wrote debug inner HTML:", f"debug_onesip_inner_{date_str}.html")

    return out_html


# ======================
# PDFìš© HTML ë˜í•‘ + CSS (ì˜ë¦¼ ë°©ì§€/ì—¬ë°±/í•œê¸€ í°íŠ¸)
# ======================
def wrap_html_for_pdf(inner_html: str) -> str:
    css = """
    @page { size: A4; margin: 14mm; }

    html, body {
      margin: 0;
      padding: 0;
      width: 100%;
      font-family: "Noto Sans CJK KR", "Noto Sans KR", "Noto Sans", sans-serif;
      font-size: 11pt;
      line-height: 1.5;
      -webkit-text-size-adjust: 100%;
    }

    * { box-sizing: border-box; }

    img, svg, video { max-width: 100% !important; height: auto !important; }
    table { width: 100% !important; max-width: 100% !important; border-collapse: collapse; }
    th, td { max-width: 100% !important; }

    div, section, article, main, header, footer {
      max-width: 100% !important;
      width: auto !important;
    }

    p, li, td, th, a, span {
      overflow-wrap: anywhere;
      word-break: break-word;
    }

    .pdf-scale {
      transform: scale(0.96);
      transform-origin: top left;
      width: 104%;
    }
    """
    return f"""<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>{css}</style>
</head>
<body>
  <div class="pdf-scale">
    {inner_html}
  </div>
</body>
</html>"""


# ======================
# RSS â†’ íƒ€ê²Ÿ ë‚ ì§œ HTML ì¶”ì¶œ
# ======================
def fetch_issue_html_for_date(target_date_kst):
    feed = feedparser.parse(RSS_URL)

    for e in feed.entries:
        if not hasattr(e, "published_parsed"):
            continue

        published_utc = datetime(*e.published_parsed[:6], tzinfo=timezone.utc)
        published_kst_date = published_utc.astimezone(KST).date()

        if published_kst_date == target_date_kst and "content" in e and e.content:
            return e.content[0].value

    return None


# ======================
# PDF ìƒì„±
# ======================
def html_to_pdf(inner_html: str, date_str: str):
    filename = f"Gmail - OneSip_{date_str}.pdf"
    final_html = wrap_html_for_pdf(inner_html)

    if DEBUG_DUMP_HTML:
        with open(f"debug_onesip_pdf_{date_str}.html", "w", encoding="utf-8") as f:
            f.write(final_html)
        print("Wrote debug pdf HTML:", f"debug_onesip_pdf_{date_str}.html")

    HTML(string=final_html).write_pdf(filename)
    return filename


# ======================
# ì´ë©”ì¼ ë°œì†¡
# ======================
def send_email(pdf_path: str, date_str: str):
    smtp_user = os.getenv("SMTP_USER")
    smtp_pass = os.getenv("SMTP_PASS")
    mail_from = os.getenv("MAIL_FROM")
    mail_to = os.getenv("MAIL_TO")

    missing = [
        k
        for k, v in {
            "SMTP_USER": smtp_user,
            "SMTP_PASS": smtp_pass,
            "MAIL_FROM": mail_from,
            "MAIL_TO": mail_to,
        }.items()
        if not v
    ]
    if missing:
        raise ValueError(f"ì´ë©”ì¼ ì„¤ì • í™˜ê²½ë³€ìˆ˜ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing)}")

    msg = EmailMessage()
    msg["Subject"] = f"{MAIL_SUBJECT_PREFIX} ({date_str})"
    msg["From"] = mail_from
    msg["To"] = mail_to

    # âœ… ë¬¸êµ¬ ì„¸ë ¨ë˜ê²Œ
    msg.set_content(
        f"{MAIL_BODY_LINE}\n\n"
        f"ì˜¤ëŠ˜ì˜ Tech Issueë¥¼ OneSipìœ¼ë¡œ ë‹´ì•˜ìŠµë‹ˆë‹¤.\n"
        f"ê°€ë³ê²Œ ì½ì–´ë³´ì‹œê³  í•˜ë£¨ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš” â˜•ï¸"
    )

    with open(pdf_path, "rb") as f:
        msg.add_attachment(
            f.read(),
            maintype="application",
            subtype="pdf",
            filename=os.path.basename(pdf_path),
        )

    context = ssl.create_default_context()
    with smtplib.SMTP_SSL(SMTP_HOST, SMTP_PORT, context=context) as server:
        server.login(smtp_user, smtp_pass)
        server.send_message(msg)


# ======================
# ë©”ì¸
# ======================
def main():
    safe_print_deepl_usage("DeepL usage(before)")

    target_date = get_target_issue_date_kst()
    date_str = target_date.strftime("%Y-%m-%d")
    print(f"Target issue date (KST): {date_str} offset: {ISSUE_OFFSET_DAYS}")

    raw_html = fetch_issue_html_for_date(target_date)
    if not raw_html:
        print("No issue found for target date.")
        return

    translated_inner_html = translate_html_preserve_layout(raw_html, date_str)

    final_text_len = len(
        BeautifulSoup(translated_inner_html, "html.parser").get_text(" ", strip=True)
    )
    print("Final HTML text length:", final_text_len)

    if final_text_len < 200:
        raise RuntimeError("Final HTML seems empty. Aborting to avoid blank PDF.")

    pdf_path = html_to_pdf(translated_inner_html, date_str)

    safe_print_deepl_usage("DeepL usage(after)")

    send_email(pdf_path, date_str)
    print("Done:", pdf_path)


if __name__ == "__main__":
    main()
